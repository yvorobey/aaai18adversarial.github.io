<!DOCTYPE html>
<html>

    <!--
    Golden Gate Image information:
       - CC0 Creative Commons licence
       - https://pixabay.com/en/golden-gate-bridge-san-francisco-388917/
    -->

    <head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109204280-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109204280-1');
</script>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title> Adversarial machine learning tutorial</title>
    <meta name="viewport" content="width=device-width">
   
    <link rel="canonical" href="index.html">

    <!-- Begin Jekyll SEO tag v2.3.0 -->
<title></title>
<meta property="og:locale" content="en_US" />
<meta name="description" content=" Adversarial machine learning tutorial" />

    <!-- Custom CSS & Bootstrap Core CSS - Uses Bootswatch Flatly Theme: http://bootswatch.com/flatly/ -->
    <link rel="stylesheet" href="css/2018_style/style.css">

    <!--Remove image dragging -->
    <BODY ondragstart="return false;" ondrop="return false;">

    <!-- Custom Fonts -->
    <link rel="stylesheet" href="css/2018_style/font-awesome/css/font-awesome.min.css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
    <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

    <!-- WEBSITE ICON IN BROWSER -->
    <!-- <link rel="icon" type="image/png" href="https://www.ieee-security.org/css/2018_style/img/icon.png"> -->

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>



    <body id="page-top" class="index">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-default-color navbar-fixed-top">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a class="page-scroll" href="index.html#">Home</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="index.html#over">Overview</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="index.html#syl">Syllabus</a>
                    </li>
                    
                    <li>
                        <a class="page-scroll" href="index.html#org">Organizer</a>
                    </li>
                    
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>

<!--
        
          
        
          
      
          
        
 -->

 

    <!-- Header -->
    <header>
        <div class="container">
            <div class="intro-text">
                <div class="intro-heading"> Adversarial machine learning tutorial <br></div>
                <!-- <div class="intro-lead-in-bold">May 24, 2018</div> -->
                <!-- <div class="intro-lead-in">co-located with the<br>39th IEEE Symposium on Security and Privacy</div> -->
                <!-- <a href="index.html#cfp" class="page-scroll btn btn-xl">Call For Papers</a> -->
            </div>
        </div>
    </header>


    <section id="over">
        <div class="container">
<!--             <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Call for Papers</h2>
                </div>
            </div> -->
            <div class="row">

<div class="text-center">
<h2>Adversarial machine learning tutorial</h2>
</div>

<h2>Overview</h2>

<p>
Machine learning has seen a remarkable rate of adoption in recent years across a broad spectrum of industries and applications. Many applications of machine learning techniques are adversarial in nature, insofar as the goal is to distinguish instances which are ``bad'' from those which are ``good''. Indeed, adversarial use goes well beyond this simple classification example: forensic analysis of malware which incorporates clustering, anomaly detection, and even vision systems in autonomous vehicles could all potentially be subject to attacks. In response to these concerns, there is an emerging literature on adversarial machine learning, which spans both the analysis of vulnerabilities in machine learning algorithms, and algorithmic techniques which yield more robust learning.

This tutorial will survey a broad array of these issues and techniques from both the cybersecurity and machine learning research areas. In particular, we consider the problems of adversarial classifier evasion, where the attacker changes behavior to escape being detected, and poisoning, where training data itself is corrupted. We discuss both the evasion and poisoning attacks, first on classifiers, and then on other learning paradigms, and the associated defensive techniques. We then consider specialized techniques for both attacking and defending neural network, particularly focusing on deep learning techniques and their vulnerabilities to adversarially crafted instances.
</p>
</div>
</div>
</section>

<section id="syl">
        <div class="container">
          <div class="row">
  <div class="text-left">

<h2>Syllabus</h2>
<br>

<dl>
<dt>9:00 am -  9:15 am</dt>    
<dt>Introduction to adversarial machine learning </dt> 
<br>
<dt>9:15 am -  10:00 am   </dt>
    <dt> Understanding evasion attacks</dt>
<dd>- <a href="https://ix.cs.uoregon.edu/~lowd/kdd05lowd.pdf">Adversarial Learning</dd>
<dd>- <a href="https://papers.nips.cc/paper/5510-feature-cross-substitution-in-adversarial-classification.pdf">Feature manipulation for evasion attacks</a></dd>
<dt> Defending against Evasion</dt>
<dd>- <a href="http://www.jmlr.org/papers/volume10/xu09b/xu09b.pdf">Robustness and regularization</a></dd>  
<dd>- <a href="http://people.csail.mit.edu/gamir/pubs/TeoGloRowSmo07.pdf">Convex learning with invariances</a></dd>  
<dd>- <a href="https://papers.nips.cc/paper/5510-feature-cross-substitution-in-adversarial-classification.pdf"> Stackelberg Game based Analysis</a></dd>
<dd>- <a href="http://vorobeychik.com/2015/randclass.pdf">Randomized Classification</a></dd>
<dd>- <a href="https://arxiv.org/abs/1708.08327">Validating Evasion Attack Models</a></dd>
<br>
<dt>10:00 am - 10:15 am </dt>
<dt>Coffee break
</dt>
<br>
<dt>10:15 am - 11:15 am</dt>

<dt> Understanding poisoning attacks </dt> 
  <!-- <dd>- Introduction to adversarial machine learning (Eugene) -->
<!-- </dd> -->
<dd>- Optimization based poisoning attack methods against</dd>
<dd>---- <a href="https://arxiv.org/abs/1608.08182.pdf">Collaborative filtering</a>, <a href="https://arxiv.org/abs/1206.6389">SVM</a>, <a href="http://pages.cs.wisc.edu/~jerryzhu/pub/Mei2015Machine.pdf">General supervised learning tasks</a></dd>
<dt>  Defense against poisoning attacks
</dt>
<dd>- <a href="https://pdfs.semanticscholar.org/79be/4aa99322ef2f3cfd1ed06f196bbf9defed4f.pdf">Robust Logistic Regression</a></dd>
<dd>- <a href="https://arxiv.org/pdf/1608.02257.pdf">Robust Sparse and PCA Regression</a></dd>
<dd>- <a href="https://arxiv.org/pdf/1706.03691.pdf">Defense Analysis against Poisoning Attacks </a></dd>

<br>

<dt>11:15 am - 11:30 am </dt> 
<dt>   Coffee break </dt>
<br>
<dt>11:30 am - 12:30 pm </dt>
<dt>Attacks on deep neural networks</dt>
<dd>- <a href="https://arxiv.org/abs/1412.6572">Adversarial Examples</a></dd>
<dd>- <a href="https://arxiv.org/abs/1608.04644"> Optimization Method for Generating Adversarial Examples</a></dd>
<dd>- <a href="https://arxiv.org/abs/1611.02770"> Delving into Transferable Adversarial Examples and Black-box attacks</a></dd>
<dd>- <a href="https://arxiv.org/abs/1801.02610">Generating Adversarial Examples with Adversarial Networks</a></dd>
<dd>- <a href="https://arxiv.org/abs/1801.02612">Spatial Transformation based Adversarial Examples
</a></dd>
<dd>- <a href="https://arxiv.org/abs/1712.09491">Exploring the Space of Black-box Attacks on Deep Neural Networks
</a></dd>
<dd>- <a href="https://arxiv.org/abs/1707.08945">Physical Adversarial Examples</a></dd>
<dd>- <a href="https://arxiv.org/pdf/1712.05526.pdf">Backdoor Poisoning Attack</a></dd>
<dd>- <a href="https://arxiv.org/pdf/1708.08689.pdf">Poisoning Attack on Deep Neural Networks</a></dd>
<dt>Defenses on deep neural networks</dt>
<dd>- <a href="https://arxiv.org/abs/1510.05328">Pre-process input: Exploring the Space of Adversarial Images
</a></dd>
 <dd>- <a href="https://arxiv.org/pdf/1511.04508.pdf">Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks
</a></dd>
<dd>- <a href="https://arxiv.org/abs/1706.06083">Iterative Adversarial Retrain</a></dd>
<dd>- <a href="https://arxiv.org/pdf/1705.07263.pdf ">Defense still has a long way: Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods
</a></dd>

<section id="org">
        <div class="container">
          <div class="row">
  <div class="text-left">

    <h2>Organizer</h2>

    <div class="instructors">
         <div class="organizer">
           <a href="mailto:crystalboli@berkeley.edu">
             <div class="instructorphoto"><img src="assets/people/boli_tumb.png"></div>
             <div>Bo Li</div>
           </a>
         </div>
         <div class="organizer">
           <a href="mailto:dawnsong@cs.berkeley.edu">
             <div class="instructorphoto"><img src="assets/people/dawnsong.jpg"></div>
             <div>Dawn Song</div>
           </a>
         </div>
         <div class="organizer">
           <a href="mailto:yevgeniy.vorobeychik@vanderbilt.edu">
             <div class="instructorphoto"><img src="assets/people/eugene_tumb.png"></div>
             <div>Yevgeniy Vorobeychik</div>
           </a>
         </div>
    </div>
</div>

</section>
<br>
For any questions, please contact the tutorial organizers at: <a href="mailto:lxbosky@gmail.com">lxbosky@gmail.com</a>
</p>

            </div>
        </div>
    </section>

        <!-- jQuery Version 1.11.0 -->
    <script src="css/2018_style/js/jquery-1.11.0.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="css/2018_style/js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="css/2018_style/js/jquery.easing.min.js"></script>
    <script src="css/2018_style/js/classie.js"></script>
    <script src="css/2018_style/js/cbpAnimatedHeader.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="css/2018_style/js/agency.js"></script>

 <footer class="site-footer">
</footer>

</body>
</html>
